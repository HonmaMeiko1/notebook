## 集群监测指标

---

### load：反应系统忙闲程度

​	系统中的load定义为 <u>特定时间间隔内运行队列中的平均线程数</u>，处于运行队列的线程满足：1 没有处于I/O等待状态；2  没有主动进入等待状态，即没有调用wait操作；3 没有被终止

load越大，代表系统CPU越繁忙，线程运行完成后等待OS分配下一个时间片的时间越长。一般来说(多核CPU的系统)，load值不大于3就是正常的，如果大于5则说明负载高。**假设：**

- CPU1分钟内最多处理100个线程任务，load值为0.2，意味着这1分钟内CPU处理了20个任务
- CPU1分钟内最多处理100个线程任务，load值为1，意味着这1分钟内CPU刚好将这100个任务处理完
- CPU1分钟内最多处理100个线程任务，load值为1.7，意味着这1分钟内CPU除了处理了这100个任务外，还有70个任务等待处理

w、top、uptime都可以查看当前系统的load，以uptime为例：

```sh
[xiaoyouqi@node01 ~]$ uptime
 11:33:34 up 41 days,  1:03,  4 users,  load average: 0.25, 0.28, 0.26
 # load average后面的三个数分别表示在过去1min，5min，15min系统的load值
```

### CPU利用率：反映CPU的使用和消耗情况

​	CPU的时间消耗主要在：用户进程、内核进程、中断处理、I/O等待、Nice时间、丢失时间、空闲等几个部分，而CPU的利用率则为这些时间所占用的总时间的百分比

使用 top 指令查看 cpu 消耗情况

```sh
[xiaoyouqi@node01 HiBench-master]$ cd ~
top - 11:39:20 up 41 days,  1:09,  4 users,  load average: 0.21, 0.26, 0.25
Tasks: 894 total,   1 running, 892 sleeping,   0 stopped,   1 zombie
%Cpu(s):  0.1 us,  0.1 sy,  0.0 ni, 99.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 26345740+total, 48964536 free, 13900792+used, 75484952 buff/cache
KiB Swap:  4194300 total,   521740 free,  3672560 used. 12351940+avail Mem 

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                          
 9620 clouder+  20   0   23.4g   2.3g 102076 S   3.6  0.9   2949:22 java                                                                             
 2278 clouder+  20   0   52.9g   2.7g  12956 S   3.0  1.1   1944:07 java                                                                             
 3144 root      20   0 5171580  31360   8248 S   1.6  0.0 261:50.45 atmon   
```

"%Cpu(s)"开头的内容便是各种状态下CPU所消耗的时间比，每一列的意思如下：

- 用户时间（User  Time）即us所对应的列，表示CPU执行用户进程所占用的时间，通常情况下希望us的占比越高越好 
- 系统时间（System Time）即sy所对应该的列，表示CPU自内核态所花费的时间，sy占比比较高通常意味着系统在某些方面设计得不合理，比如频繁的系统调用导致的用户态和内核态的频繁切换
- Nice时间（Nice Time）即ni所对应的列，表示系统在调整进程优先级的时候所花费的时间
- 空闲时间（Idle Time）即id所对应的列，表示系统处于空闲期，等待进程运行，这个过程所占用的时间。当然，我们希望id的占比越低越好
- 等待时间（Waiting Time）即wa所对应的列，表示CPU在等待I/O操作所花费的时间，系统不应该花费大量的时间来进行等待，否则便表示可能有某个地方设计不合理
- 硬件中断处理时间（Hard Irq Time）即hi对应的列，表示系统处理硬件中断所占用的时间
- 软件中断处理时间（Soft Irq Time）即si对应的列，表示系统处理软件中断所占用的时间
- 丢失时间（Steal Time）即st对应的列，实在硬件虚拟化开始流行后操作系统新增的一列，表示被强制等待虚拟CPU的时间

对于多个或多核CPU的情况，可以按1查看每个CPU的利用情况，便可以查看到每个核的CPU利用率：

```
[xiaoyouqi@node01 ~]$ top
top - 11:41:00 up 41 days,  1:10,  4 users,  load average: 0.18, 0.22, 0.23
Tasks: 897 total,   1 running, 895 sleeping,   0 stopped,   1 zombie
%Cpu0  :  0.3 us,  0.3 sy,  0.0 ni, 99.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu1  :  0.0 us,  0.3 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
......
%Cpu23 :  0.7 us,  0.0 sy,  0.0 ni, 99.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
%Cpu24 :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
```

### 内存使用

Linux的内存包括物理内存Mem和虚拟内存Swap

```sh
[xiaoyouqi@node01 ~]$ free -m
              total        used        free      shared  buff/cache   available
Mem:         257282      135750       47803          44       73728      120623
Swap:          4095        3586         509
```

- total----内存总共的大小
- used----已使用的内存大小
- free----可使用的内存大小
- shared----多个进程共享的内存大小
- buffers----缓冲区的大小
- cached----缓存的大小

#### swap I/O

```
[xiaoyouqi@node01 ~]$ vmstat
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0 3671880 52265792 372676 71812544    0    0     0     4    0    0  0  0 100  0  0
```

si表示每秒从磁盘交换到内存的数据量，单位是KB/s，so表示每秒从内存交换到磁盘的数据量，单位也是KB/s

### 磁盘剩余空间

```sh
[xiaoyouqi@node01 ~]$ df -h
文件系统                 容量  已用  可用 已用% 挂载点
devtmpfs                 126G     0  126G    0% /dev
tmpfs                    126G  8.0K  126G    1% /dev/shm
tmpfs                    126G   53M  126G    1% /run
tmpfs                    126G     0  126G    0% /sys/fs/cgroup
/dev/mapper/centos-root  440G  137G  303G   32% /
/dev/sda2               1014M  181M  834M   18% /boot
/dev/sda1                200M   12M  189M    6% /boot/efi
/dev/sdc                 7.3T   21G  6.9T    1% /data2
/dev/sdb                 7.3T   23G  6.9T    1% /data1
/dev/sdd                 7.3T   23G  6.9T    1% /data3
cm_processes             126G   54M  126G    1% /run/cloudera-scm-agent/process
tmpfs                     26G   40K   26G    1% /run/user/1012
tmpfs                     26G     0   26G    0% /run/user/1013
tmpfs                     26G     0   26G    0% /run/user/0
tmpfs                     26G     0   26G    0% /run/user/1010
```

### 磁盘 I/O

```
[xiaoyouqi@node01 ~]$ iostat
Linux 3.10.0-1160.118.1.el7.x86_64 (node01) 	2024年09月05日 	_x86_64_	(96 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.33    0.00    0.10    0.00    0.00   99.58

Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
sda              10.48         2.63       323.37    9340546 1147154869
sdd               0.45         5.70        13.98   20227537   49588552
sdc               0.67         5.45        15.67   19344141   55602220
sdb               0.67         4.68        15.88   16603657   56328020
dm-0             10.91         2.48       322.19    8793336 1142957604
dm-1              0.33         0.14         1.18     509680    4195008
```

Device表示设备名称、tps表示每秒处理的I/O请求数、kB_read/s表示每秒从设备读取的数据量、kB_wrtn/s表示每秒向设备写入的数据量、kB_read表示读取的数据总量、kB_wrtn表示写入的数据总量

---

## 使用RAPL获取集群cpu功耗

### 思路：

1. 首先使用cpuid代码或者lscpu查看cpu是否支持msr （这个一般都支持）
2. 读取MSR_RAPL_POWER_UNIT寄存器的内容，得到能量的基本计算单元。
3. 读取MSR_PKG_ENERGY_STATUS寄存器的内容，用来乘以能量基本计算单元，得到具体的累积能量消耗（单位：焦耳）。
4. 程序暂停一段时间。
5. 再次读取MSR_PKG_ENERGY_STATUS寄存器的内容，乘以能量基本计算单元，得到累积能量消耗。
6. 用第二次得到的累积能量消耗减去第一次的，除以时间，就得到了这段时间的平均功耗。

详细的代码在github上：https://github.com/HonmaMeiko1/intel_powerMonitor.git

### 查看服务器超线程 CPU-core 归属的物理 CPU (socket)

#### 安装 numctl

为服务器安装能够查看虚拟cpu对应的socket的工具包，这里我选择的是 numactl。由于服务器的linux版本为centOS 7，版本太过老旧，无法直接使用 sudo yum install numactl (会显示找不到官网镜像网站，官网服务器停止对centOS 7的维护，能找到的rpm包都是针对centOS 8或9的)，因此可以曲线救国：

1. 去github下载 numctl 的 v2.0.12，网址如下：[Releases · numactl/numactl (github.com)](https://github.com/numactl/numactl/releases)

2. 解压.tar.gz文件

   ```sh
   tar -zxvf numactl-2.0.12.tar.gz
   ```

3. 构建配置环境

   ```sh
   cd numactl-2.0.12
   ./configure
   ```

4. 编译软件

   ```sh
   cd numactl-2.0.12
   make
   ```

5. 安装软件

   ```sh
   cd numactl-2.0.12
   sudo make install
   ```

#### 查看cpu所属socket

执行 `numactl --hardware` 可以查看硬件对 NUMA 的支持信息：

```
[xiaoyouqi@node01 numactl-2.0.12]$ numactl --hardware
available: 2 nodes (0-1)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
node 0 size: 128282 MB
node 0 free: 583 MB
node 1 cpus: 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
node 1 size: 129000 MB
node 1 free: 70 MB
node distances:
node   0   1 
  0:  10  21 
  1:  21  10 
```

1. CPU 被分成 node 0 和 node 1 两组（这台机器有两个 CPU Socket）。

2. 一组 CPU 分配到 128 GB 的内存（这台机器总共有 256GB 内存）。

3. node distances 是一个二维矩阵，node[i][j] 表示 node i 访问 node j 的内存的相对距离。比如 node 0 访问 node 0 的内存的距离是 10，而 node 0 访问 node 1 的内存的距离是 21。

4. 可见一台机器上是存在2个socket(物理CPU)，每个CPU24个core，每个core两个线程，也可以通过lscpu验证：

   ```
   [xiaoyouqi@node01 numactl-2.0.12]$ lscpu
   Architecture:          x86_64
   CPU op-mode(s):        32-bit, 64-bit
   Byte Order:            Little Endian
   CPU(s):                96
   On-line CPU(s) list:   0-95
   Thread(s) per core:    2
   Core(s) per socket:    24
   座：                 2
   NUMA 节点：         2
   厂商 ID：           GenuineIntel
   CPU 系列：          6
   型号：              85
   型号名称：        Intel(R) Xeon(R) Platinum 8168 CPU @ 2.70GHz
   步进：              4
   CPU MHz：             2700.000
   CPU max MHz:           3700.0000
   CPU min MHz:           1200.0000
   BogoMIPS：            5400.00
   虚拟化：           VT-x
   L1d 缓存：          32K
   L1i 缓存：          32K
   L2 缓存：           1024K
   L3 缓存：           33792K
   NUMA 节点0 CPU：    0-23,48-71
   NUMA 节点1 CPU：    24-47,72-95
   ```

- numactl 命令还有几个重要选项：

1. `--cpubind=0`： 绑定到 node 0 的 CPU 上执行。
2. `--membind=1`： 只在 node 1 上分配内存。
3. `--interleave=nodes`：[nodes](https://zhida.zhihu.com/search?q=nodes&zhida_source=entity&is_preview=1) 可以是 all、N,N,N 或 N-N，表示在 nodes 上轮循（round robin）分配内存。
4. `--physcpubind=cpus`：cpus 是 /proc/cpuinfo 中的 processor（[超线程](https://zhida.zhihu.com/search?q=超线程&zhida_source=entity&is_preview=1)） 字段，cpus 的格式与 --interleave=nodes 一样，表示绑定到 [cpus](https://zhida.zhihu.com/search?q=cpus&zhida_source=entity&is_preview=1) 上运行。
5. `--preferred=1`： 优先考虑从 node 1 上分配内存。

由此可以得出结论，服务器的一台机器是有两个物理CPU，每个CPU24个核，开启了超线程，每个核有两个线程

